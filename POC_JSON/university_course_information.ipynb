{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from typing import List, Dict, Any\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import PyPDF2\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up OpenRouter with OpenAI client\n",
    "OPENROUTER_API_KEY = \"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\"\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    ")\n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "embeddings_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Connect to Qdrant (local or cloud)\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://8b6857da-0682-417b-a31b-2a83bef2cab3.us-east-1-0.aws.cloud.qdrant.io\",\n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.dWjs7ZnPcyo0lbk1tvelYBim14HKNwDm1qfWTKaoVoQ\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract text from PDF and split into chunks with metadata.\"\"\"\n",
    "    print(f\"Processing PDF file: {pdf_path}\")\n",
    "    \n",
    "    # Open the PDF file\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "        print(f\"PDF has {num_pages} pages\")\n",
    "        \n",
    "        # Extract text from each page\n",
    "        all_text = \"\"\n",
    "        for i in range(num_pages):\n",
    "            page = reader.pages[i]\n",
    "            text = page.extract_text()\n",
    "            all_text += text + \"\\n\\n\"\n",
    "    \n",
    "    # Split the text into chunks of reasonable size (~1000 characters)\n",
    "    chunks = []\n",
    "    \n",
    "    # Split by sections/paragraphs (adjust as needed based on PDF structure)\n",
    "    raw_chunks = re.split(r'\\n\\s*\\n', all_text)\n",
    "    \n",
    "    current_chunk = \"\"\n",
    "    for chunk in raw_chunks:\n",
    "        if not chunk.strip():\n",
    "            continue\n",
    "            \n",
    "        if len(current_chunk) + len(chunk) < 1000:\n",
    "            current_chunk += chunk + \"\\n\\n\"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                # Extract potential keywords (simple heuristic: capitalized words)\n",
    "                keywords = re.findall(r'\\b[A-Z][A-Za-z]{2,}\\b', current_chunk)\n",
    "                keywords = list(set([k for k in keywords if k.lower() not in stopwords.words('english')]))[:10]\n",
    "                \n",
    "                chunks.append({\n",
    "                    \"text\": current_chunk.strip(),\n",
    "                    \"keywords\": keywords\n",
    "                })\n",
    "            current_chunk = chunk + \"\\n\\n\"\n",
    "    \n",
    "    # Add the last chunk if not empty\n",
    "    if current_chunk.strip():\n",
    "        keywords = re.findall(r'\\b[A-Z][A-Za-z]{2,}\\b', current_chunk)\n",
    "        keywords = list(set([k for k in keywords if k.lower() not in stopwords.words('english')]))[:10]\n",
    "        \n",
    "        chunks.append({\n",
    "            \"text\": current_chunk.strip(),\n",
    "            \"keywords\": keywords\n",
    "        })\n",
    "    \n",
    "    print(f\"Split PDF into {len(chunks)} chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Handling and Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Read JSON file and return its contents.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def generate_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Generate embedding for a text using OpenAI's text-embeddings-small-3 model.\"\"\"\n",
    "    start_time = time.time()\n",
    "    response = embeddings_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    embedding_time = time.time() - start_time\n",
    "    print(f\"Embedding time: {embedding_time} seconds\")\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def create_payload(entry: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Create a payload with text, keywords, and metadata for Qdrant.\"\"\"\n",
    "    text = entry.get(\"text\", \"\")\n",
    "    keywords = entry.get(\"keywords\", [])\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"keywords\": keywords,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qdrant Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection(collection_name: str, vector_size: int = 1536):\n",
    "    \"\"\"Create a collection in Qdrant if it doesn't exist.\"\"\"\n",
    "    try:\n",
    "        qdrant_client.get_collection(collection_name)\n",
    "        print(f\"Collection {collection_name} already exists\")\n",
    "    except Exception:\n",
    "        qdrant_client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=models.VectorParams(\n",
    "                size=vector_size,\n",
    "                distance=models.Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "        print(f\"Created collection {collection_name}\")\n",
    "\n",
    "def process_and_upload_data(data: List[Dict[str, Any]], collection_name: str):\n",
    "    \"\"\"Process each entry, generate embedding, and upload to Qdrant.\"\"\"\n",
    "    batch_size = 10  # Process in batches to avoid API rate limits\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "\n",
    "        points = []\n",
    "        for j, entry in enumerate(batch):\n",
    "            # Create payload with text and keywords\n",
    "            payload = create_payload(entry)\n",
    "\n",
    "            # Generate embedding for text content\n",
    "            embedding = generate_embedding(entry[\"text\"])\n",
    "\n",
    "            # Add to points\n",
    "            points.append(models.PointStruct(\n",
    "                id=i+j,\n",
    "                vector=embedding,\n",
    "                payload=payload\n",
    "            ))\n",
    "\n",
    "        # Upload batch to Qdrant\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=points\n",
    "        )\n",
    "\n",
    "        print(f\"Uploaded batch {i//batch_size + 1}/{(len(data) + batch_size - 1)//batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Processing and Uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_and_upload(pdf_path: str, collection_name: str = \"admission_course_guide\"):\n",
    "    \"\"\"Process a PDF file and upload its embeddings to Qdrant.\"\"\"\n",
    "    # Extract text from PDF\n",
    "    pdf_data = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Create collection\n",
    "    create_collection(collection_name)\n",
    "    \n",
    "    # Process and upload data\n",
    "    process_and_upload_data(pdf_data, collection_name)\n",
    "    \n",
    "    print(f\"PDF {pdf_path} processed and uploaded to Qdrant collection {collection_name}\")\n",
    "    return pdf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append PDF to Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_pdf_to_collection(pdf_path: str, collection_name: str = \"admission_course_guide\"):\n",
    "    \"\"\"Process a PDF file and append its embeddings to an existing Qdrant collection.\"\"\"\n",
    "    # Extract text from PDF\n",
    "    pdf_data = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Verify collection exists\n",
    "    try:\n",
    "        collection_info = qdrant_client.get_collection(collection_name)\n",
    "        print(f\"Found existing collection {collection_name}\")\n",
    "    except Exception:\n",
    "        print(f\"Collection {collection_name} does not exist, creating it...\")\n",
    "        create_collection(collection_name)\n",
    "    \n",
    "    # Get the count of existing points to avoid ID conflicts\n",
    "    collection_info = qdrant_client.get_collection(collection_name)\n",
    "    existing_points = 12\n",
    "    print(f\"Collection has {existing_points} existing points\")\n",
    "    \n",
    "    # Process in batches to avoid API rate limits\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(pdf_data), batch_size):\n",
    "        batch = pdf_data[i:i+batch_size]\n",
    "        \n",
    "        points = []\n",
    "        for j, entry in enumerate(batch):\n",
    "            # Create payload with text and keywords\n",
    "            payload = create_payload(entry)\n",
    "            \n",
    "            # Generate embedding for text content\n",
    "            embedding = generate_embedding(entry[\"text\"])\n",
    "            \n",
    "            # Add to points with offset IDs to avoid conflicts\n",
    "            points.append(models.PointStruct(\n",
    "                id=existing_points + i + j,\n",
    "                vector=embedding,\n",
    "                payload=payload\n",
    "            ))\n",
    "        \n",
    "        # Upload batch to Qdrant\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=points\n",
    "        )\n",
    "        \n",
    "        print(f\"Uploaded batch {i//batch_size + 1}/{(len(pdf_data) + batch_size - 1)//batch_size}\")\n",
    "    \n",
    "    print(f\"PDF {pdf_path} processed and appended to Qdrant collection {collection_name}\")\n",
    "    return pdf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_qdrant_simple(query: str, collection_name: str, limit: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Perform simple search in Qdrant for a single query.\"\"\"\n",
    "    # Generate embedding for the query\n",
    "    embedding = generate_embedding(query)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Perform search\n",
    "    search_results = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=embedding,\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "        score_threshold=0.4\n",
    "    )\n",
    "    print(search_results)\n",
    "    search_time = time.time() - start_time\n",
    "    print(f\"Search time: {search_time} seconds\")\n",
    "    \n",
    "    start_time_1 = time.time()\n",
    "    results = []\n",
    "    for scored_point in search_results.points:\n",
    "        results.append({\n",
    "            \"id\": scored_point.id,\n",
    "            \"score\": scored_point.score,\n",
    "            \"payload\": scored_point.payload\n",
    "        })\n",
    "    format_time = time.time() - start_time_1\n",
    "    print(f\"Format time: {format_time} seconds\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query: str, context: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Generate a response using OpenAI based on retrieved context.\"\"\"\n",
    "    # Prepare context text from search results\n",
    "    start_time = time.time()\n",
    "    context_text = \"\\n\\n\".join([\n",
    "        f\"Document {i+1}:\\nText: {item['payload']['text']}\\nKeywords: {', '.join(item['payload']['keywords'])}\"\n",
    "        for i, item in enumerate(context)\n",
    "    ])\n",
    "    context_time = time.time() - start_time\n",
    "    print(f\"Context time: {context_time} seconds\")\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    You are an authoritative academic assistant for Notre Dame University (NDU) providing precise information based on the retrieved documents.\n",
    "\n",
    "    IMPORTANT GUIDELINES:\n",
    "    1. Provide ONLY ONE definitive answer based on the highest relevance matches in the context.\n",
    "    2. If multiple potential answers exist, choose the one with the strongest evidence in the retrieved documents.\n",
    "\n",
    "    Your goal is to provide the single most accurate answer as if you were an official university representative.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"Question: {query}\\n\\nContext:\\n{context_text}\"\n",
    "    start_time_1 = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"openai/gpt-4o\",  # Using a powerful model for response generation\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    response_time = time.time() - start_time_1\n",
    "    print(f\"Response time: {response_time} seconds\")\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline_simple(query: str, collection_name: str = \"admission_course_guide\"):\n",
    "    \"\"\"Complete RAG pipeline from user query to response.\"\"\"\n",
    "    print(f\"Original query: {query}\")\n",
    "\n",
    "    # Search Qdrant with a single query\n",
    "    search_results = search_qdrant_simple(query, collection_name, limit=3)\n",
    "\n",
    "    # Generate response\n",
    "    response = generate_response(query, search_results)\n",
    "\n",
    "    return {\n",
    "        \"original_query\": query,\n",
    "        \"search_results\": search_results,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage - Process PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process a PDF file and upload to Qdrant\n",
    "pdf_path = \"../data/raw/admission guide.pdf\"  # Update with your PDF path\n",
    "\n",
    "# Check if the PDF file exists\n",
    "if os.path.exists(pdf_path):\n",
    "    # Process the PDF and upload to Qdrant\n",
    "    processed_data = process_pdf_and_upload(pdf_path)\n",
    "    print(\"PDF processed and uploaded successfully\")\n",
    "else:\n",
    "    print(f\"Error: PDF file not found at {pdf_path}\")\n",
    "    print(\"Please specify the correct path to your PDF file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage - Append Another PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to append another document to the same collection:\n",
    "second_pdf_path = \"../data/raw/Nazir Hawi.pdf\"\n",
    "if os.path.exists(second_pdf_path):\n",
    "    append_pdf_to_collection(second_pdf_path)\n",
    "    print(\"Second PDF appended successfully\")\n",
    "else:\n",
    "    print(f\"Warning: Second PDF file not found at {second_pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage - Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: How many credits does a computer science major have?\n",
      "Embedding time: 0.8634097576141357 seconds\n",
      "points=[ScoredPoint(id=14, version=5, score=0.64623094, payload={'text': 'Bachelor of Science in Computer Science - ABET Accredited Program\\nCSC 450 Human-Computer Interaction (3 credits)\\nDescription: Design and evaluation of user interfaces, usability principles, and user experience design.\\nPrerequisite: CSC 305 System Analysis and Design.\\nCSC 463 Advanced Software Development (3 credits)\\nDescription: Techniques and practices for building large-scale, maintainable software systems.\\nPrerequisite: CSC 305 System Analysis and Design.\\nCSC 480 Internship (3 credits)\\nDescription: Practical experience in the IT field to apply academic knowledge in a professional environment.\\nPrerequisite: Completion of at least 90 credits.\\nCSC 490 Senior Study (3 credits)\\nDescription: Capstone project involving research, development, and presentation in a specialized computing\\narea.\\nPrerequisite: Final year standing and departmental approval.\\nLiberal Arts Curriculum\\nENG 101 English Composition (3 credits)\\nDescription: Development of writing skills, including grammar, composition, and research paper techniques.\\nPrerequisite: None.\\nMTH 110 Pre-Calculus Mathematics (3 credits)\\nDescription: Fundamental concepts in algebra and trigonometry as preparation for calculus.', 'keywords': ['Advanced', 'Prerequisite', 'Human', 'Computer', 'Description', 'Arts', 'Techniques', 'Development', 'ENG', 'Program']}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=12, version=3, score=0.6060892, payload={'text': 'Bachelor of Science in Computer Science - ABET Accredited Program\\nCSC 226 Introduction to Database (3 credits)\\nDescription: Fundamentals of database systems, data modeling, relational databases, and SQL.\\nPrerequisite: Introduction to Programming.\\nCSC 305 System Analysis and Design (3 credits)\\nDescription: Study of system development life cycle (SDLC), requirements gathering, analysis, and design\\ntechniques.\\nPrerequisite: CSC 226 Introduction to Database.\\nCSC 312 Computer Architecture (3 credits)\\nDescription: Organization and structure of computer systems, instruction sets, and assembly language\\nprogramming.\\nPrerequisite: Introduction to Computer Science.\\nCSC 316 Fundamentals of Computer Security (3 credits)\\nDescription: Basic concepts of securing systems, cryptography, network security, and threat mitigation.\\nPrerequisite: CSC 312 Computer Architecture.\\nCSC 317 Information Assurance and Security (3 credits)\\nDescription: Strategies to ensure data integrity, availability, and confidentiality within IT environments.\\nPrerequisite: CSC 316 Fundamentals of Computer Security.\\nCSC 345 Fundamentals of Computer Network Management (3 credits)\\nDescription: Principles of managing computer networks, including protocols, architectures, and network', 'keywords': ['Prerequisite', 'Information', 'Computer', 'Programming', 'Description', 'Principles', 'Program', 'Database', 'System', 'Management']}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=13, version=4, score=0.6012268, payload={'text': 'Bachelor of Science in Computer Science - ABET Accredited Program\\ntroubleshooting.\\nPrerequisite: CSC 312 Computer Architecture.\\nCSC 385 Internet Computing (3 credits)\\nDescription: Technologies behind web applications, internet protocols, and web development principles.\\nPrerequisite: CSC 226 Introduction to Database.\\nCSC 405 Systems Integration (3 credits)\\nDescription: Combining different computing systems and software applications to function as a coordinated\\nwhole.\\nPrerequisite: CSC 305 System Analysis and Design.\\nCSC 414 Applied Operating Systems (3 credits)\\nDescription: In-depth study of operating system concepts, including process management, memory\\nmanagement, and file systems.\\nPrerequisite: CSC 312 Computer Architecture.\\nCSC 425 Data Communications and Computer Networks (3 credits)\\nDescription: Covers transmission methods, network design, TCP/IP protocols, and wireless communication.\\nPrerequisite: CSC 345 Fundamentals of Computer Network Management.\\nCSC 446 Applied Database Systems (3 credits)\\nDescription: Advanced database concepts, database design, and database-driven application development.\\nPrerequisite: CSC 226 Introduction to Database.', 'keywords': ['Advanced', 'TCP', 'Prerequisite', 'Computer', 'Description', 'Data', 'Applied', 'Internet', 'Program', 'Systems']}, vector=None, shard_key=None, order_value=None)]\n",
      "Search time: 3.9852256774902344 seconds\n",
      "Format time: 0.0 seconds\n",
      "Context time: 0.0 seconds\n",
      "Response time: 3.1622016429901123 seconds\n",
      "Total time taken: 8.01184606552124 seconds\n",
      "\n",
      "Final Response:\n",
      "The retrieved documents do not specify the total number of credits required for a Bachelor of Science in Computer Science at Notre Dame University. They provide information on individual courses and their credit values, but not the overall credit requirement for the major.\n"
     ]
    }
   ],
   "source": [
    "# Test the pipeline with a sample query\n",
    "start_time = time.time()\n",
    "result = rag_pipeline_simple(\"How many credits does a computer science major have?\")\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time} seconds\")\n",
    "\n",
    "# Display the response\n",
    "print(\"\\nFinal Response:\")\n",
    "print(result[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_variations(query: str) -> List[str]:\n",
    "    \"\"\"Generate variations of the query using OpenAI.\"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    Create one alternative versions of the user's query. \n",
    "    Each version should:\n",
    "    1. Maintain the original meaning\n",
    "    2. Use different wording or phrasing\n",
    "    3. Be a complete, well-formed question\n",
    "    \n",
    "    Return ONLY two variations, one per line, with no additional text.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"openai/gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    variations_text = response.choices[0].message.content\n",
    "    variations = [line.strip() for line in variations_text.split('\\n') if line.strip()]\n",
    "    \n",
    "    # Ensure we have exactly 2 variations\n",
    "    if len(variations) > 1:\n",
    "        variations = variations[:1]\n",
    "    while len(variations) < 1:\n",
    "        variations.append(query)  # Use original query as fallback\n",
    "        \n",
    "    return variations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-kuwait-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
